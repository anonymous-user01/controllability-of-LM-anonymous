{"task": "rl", "behavior_model": "gpt2_small", "dataset": "topic-3", "rl_lr": 0.0005, "batch_size": 256, "num_epoch": 20, "decoding": "top-k", "prefix_len": 2, "gen_len": 15, "dropout": "random", "dropout_rate": 0.9}